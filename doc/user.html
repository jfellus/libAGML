<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <!--[if lt IE 9]>
    <script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
  <![endif]-->
  <link rel="stylesheet" href="style/a.css">
</head>
<body>
<h1 id="libagml-users-documentation">libAGML User's Documentation</h1>
<p><r>libAGML</r> is a software library for Asynchronous Decentralized Machine Learning.</p>
<p>It is written in C++ and is (initially) targetted to POSIX platforms. It allows a set of computers in a network to run optimization algorithms and statistical learning chains in a fully decentralized and asynchronous fashion.</p>
<ul>
<li><a href="#philo">Philosophy</a></li>
<li><a href="#features">Key features</a></li>
<li><a href="#archi">Architecture</a></li>
</ul>
<h2 id="philosophy"><a name="philo"></a>Philosophy</h2>
In an ideal world, people solving computationally intensive problems would like to take advantage of any available computing and memory resources to speed-up the process or to simply make the problem practically tractable. However, in the real world, computation power is limited to a single processor and a main memory, unless one uses a parallel and/or distributed implementation of his favorite algorithm. Adapting algorithms to distributed setups is a tedious and somehow hazardous job, that can turn into dirty-complicated-unreadable-sub-optimal networking code. As engineers and researchers get frustrated, modern computers get sad of being both very expensive and most often underused. And last but not least, distributed implementations always raise open research questions that take decades to only be partially tackled, even for the simplest case of computing an average...
<div style="text-align:right;font-size:14px;">
<em>(for a foretaste, take a look at John Tsitsiklis' work at http://www.mit.edu/~jnt/flocking.html)</em>
</div>
<h4 id="shortcomings-of-classical-distributed-algorithms">Shortcomings of classical distributed algorithms</h4>
<p>A typical Machine Learning or Pattern Recognition task involves a bunch of algorithms with different objectives, skilfully chained to obtain the desired output from given input data. Such learning models are often defined regardless of where input data will come from, how many processors and memories will be used, and how this material will be connected together. In fact, the conceptual model is intented to work independently of these deployment considerations. Sadly, this is rarely the case in practice, as beyond trivial configurations the distributed algorithm either performs badly, slowly, becomes instable or simply crashes. The main reasons for such disappointments are:</p>
<ol type="1">
<li><p><strong>Scaling issues</strong> : When data amounts, number of involved computers, and number of parameters simultaneously explode, the straightforward distributed formulation of an existing centralized algorithm is likely to exhibit unexcepted and probably undesired new &quot;behaviors&quot;, if a careful asymptotic analysis wasn't carried out.</p></li>
<li><p><strong>Robustness issues</strong> : Computer-Computer communications are notably much less reliable than Processor-Memory communications. While the latter benefits from great integration efforts from computer manufacturers, the former is mostly left to the programmer's hands. Providing efficient, fast, and robust Computer-Computer communications is more often a subjective matter of arounsing tradeoffs than an objective question of absolute talent. Could the end-user afford a high-end terabit switch, remote data exchanges will always be significantly slower and more unreliable than direct processor-memory transfers in an individual computer (at least because sending a message on the network has a memory transfer and I/O interruptions overhead).</p></li>
<li><p><strong>Synchronisation issues</strong> : Following the Turing/Von Neumann computation paradigm that grounds traditional algorithmic designs, distributed implementations are usually defined as a <em>sequence</em> of operations. Some of them are purely local, but some of them involve communications between (perhaps many) computers. The result of some step is only coherent when <em>every</em> computer involved in this step is done with its own computations and communications. Thus, all computers have to synchronize their operations to proceed to the next steps. This has harmful consequences on runtime for small-scale setups, but also entail dramatic bottlenecks when scaling up to large networks and long-running operations due to massive input data.</p></li>
<li><p><strong>Centralization issues</strong> : A problem arising in distributed optimization algorithms is that contrarily to centralized algorithms, each processor is generally associated with a separate memory. In particular, there is no memory that would be common to all computation units. A choice must then be made between two alternatives: (i) Keeping a single set of updated parameters at a single location and using communications to gather data generated by each processors to this central site. (ii) Allocating one set of parameter <em>per processor</em>. Choosing the first solution, we put a great reponsibility on the computer chosen as the central site. In case of weakness of this computer (failure, clogging, crash), the whole system fails. Choosing the second one, we need to define a mechanism that produce a single set of final parameters out of multiple ones. This mechanism can't be untrusted to a central coordinator, otherwise we fall back to the first solution. Hence, we must consider that all units play the same role in producing the final result, in a fully decentralized fashion.</p></li>
</ol>
<p>In real situations, the above-mentioned shortcomings combine into a nameless computational nightmare. As certainties vanish, building upon these constraints opens up a different way of thinking computation as a decentralized, asynchronous and dynamic process resulting from a collective effort involving a population of individual entities with independent lifecycles, internal parameters, and timeline, as well as autonomous interaction capacities. After all, that's how computation first appears to the external observer of any natural phenomenon: an asynchronous dynamic system with high structural and topological plasticity without any central coordination on top of it (unless otherwise proven... ;-) )</p>
<h4 id="key-features"><a name="features"></a>Key features</h4>
<p>To overcome the four above-mentioned drawbacks, <r>libAGML</r> builds on three key features:</p>
<ul>
<li><p><strong>Decentralization</strong> : In <r>libAGML</r>, all computation units play the same role. That is, there isn't any <em>master node</em> and no specific connection topology is required. Each algorithm is run through a set of <code>Node</code>s grouped into functionally homogeneous <code>NodeGroup</code>s, meaning that all <code>Node</code>s in a given <code>NodeGroup</code> execute the very same algorithm on its own local data subset, and communicate with its direct neighbors without any distinction. In practice, when two <code>NodeGroup</code> running different algorithms need to exchange data, output nodes are never aware of which input node they are talking to. They only know that they are sending data to one of the nodes in the destination <code>NodeGroup</code>. Consequently, data exchanges between algorithms is always conceived at the <code>NodeGroup</code> level, never at the individual <code>Node</code> level.</p></li>
<li><p><strong>Asynchrony</strong> : <r>libAGML</r> is mostly built on <em>asynchronous communications</em>. This means that when some node sends data to a neighbor, it is not allowed (or at least discouraged) to wait for the neighbor's response. The advantage of enforcing such constraint appears when receiver nodes are busy or require a long running operation to process incoming data before answering to the sender. Any algorithm or protocol that is built on synchronous communications (<em>i.e.</em> where senders wait for receiver response before doing any further computation) is likely to encounter dead-locks or catastrophic slow-downs when fast processing of incoming data and nodes availability is not always formally guaranteed. This is actually the case in most optimization algorithm in machine learning where updating parameters both locally and in reaction to incoming data can not be carried out simultaneously, thus requiring synchronisation barriers between two long-running operations. Asynchronous communications circumvent this drawback, but generally require dedicated algorithmic schemes, thus tagged as <em>asynchronous</em>.</p></li>
<li><p><strong>Plasticity</strong> : <r>libAGML</r> is designed to handle <em>dynamic networks</em>. That is, the number of available processors, the network connectivity, the computations we want to assign to each processor, the implemented algorithms chain, the nature and location of the data sources can vary at any time. Thus, algorithms using <r>libAGML</r> have to take into account this environmental dynamics by offering convergence and robustness guarantees under such a regime.</p></li>
</ul>
<h2 id="architecture"><a name="archi"></a>Architecture</h2>
<figure>
<img src="img/archi.png" title="libAGML Architecture" alt="libAGML Architecture" /><figcaption>libAGML Architecture</figcaption>
</figure>
<p><r>libAGML</r> architecture is made of three description layers :</p>
<ul>
<li><p><strong>The Model Layer</strong> : On this layer, the user declares <code>NodeGroup</code>s and makes connections between them. Each <code>NodeGroup</code> contains a given number of <code>Node</code>s that execute the same algorithm in parallel on node-local subsets of input data. Connecting <code>NodeGroup1</code> to <code>NodeGroup2</code> means that any <code>Node</code> in <code>NodeGroup1</code> will eventually send data to any <code>Node</code> in <code>NodeGroup2</code>. Typically, target nodes will be drawn at random using one of the methods provided in class <code>Node</code>. Nevertheless, neighbors can also be selected in a deterministic way through a neighbor id number, albeit without precisely knowing in advance which destination node will be associated with a given neighbor id.</p></li>
<li><p><strong>The Computation Layer</strong> : On this layer, the user declares <code>Host</code>s available for computations. An <code>Host</code> is identified by an IP address and a port number, and declares one or more <code>Thread</code>s available to schedule <code>Node</code>s in parallel.</p></li>
<li><p><strong>The Association Layer</strong> : This intermediary layer makes the link between the Model Layer and the Computation Layer, by assigning any <code>Node</code> to any <code>Host</code>. In addition, one may explicitly assign a set of <code>Node</code> to a same <code>Thread</code>.</p></li>
</ul>
</body>
</html>
